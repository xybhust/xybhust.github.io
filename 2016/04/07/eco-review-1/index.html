<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Review of Econometrics (1) | Yibing Xie</title>
  <meta name="author" content="Yibing Xie">
  
  <meta name="description" content="This article summarizes simple linear regression, which covers chapter 2-4 from the Principles of Econometrics 4th Edition.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Review of Econometrics (1)"/>
  <meta property="og:site_name" content="Yibing Xie"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Yibing Xie" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-76123207-1', 'auto');
	ga('send', 'pageview');

</script>


</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Yibing Xie</a></h1>
  <h2><a href="/">A place with no name..</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-07T17:34:50.000Z"><a href="/2016/04/07/eco-review-1/">2016-04-07</a></time>
      
      
  
    <h1 class="title">Review of Econometrics (1)</h1>
  

    </header>
    <div class="entry">
      
        <p>This article summarizes simple linear regression, which covers chapter 2-4 from the <a href="http://www.amazon.com/Principles-Econometrics-R-Carter-Hill/dp/0470626739/ref=sr_1_1?ie=UTF8&amp;qid=1460072940&amp;sr=8-1&amp;keywords=econometrics+principles" target="_blank" rel="external"><em><strong>Principles of Econometrics</strong></em></a> 4th Edition.<br><a id="more"></a><br><!-- Chapter 2--></p>
<h2 id="The-Simple-Linear-Regression-Model"><a href="#The-Simple-Linear-Regression-Model" class="headerlink" title="The Simple Linear Regression Model"></a>The Simple Linear Regression Model</h2><p><strong>GAUSS–MARKOV ASSUMPTION:</strong><br><strong>SR1.</strong> The value of $y$, for each value of $x$, is<br>$$ y = \beta_1 + \beta_2x + e $$</p>
<p><strong>SR2.</strong> The expected value of the random error $e$ is<br>$$ E\left(e \right ) = 0 $$<br>which is equivalent to assuming that<br>$$ E\left(y \right) = \beta_1 + \beta_2x $$</p>
<p><strong>SR3.</strong> The variance of the random error $e$ is<br>$$ \text{var}\left(e \right ) = \sigma^2 = \text{var}\left(y \right ) $$</p>
<p><strong>SR4.</strong> The covariance between any pair of random errors $e_i$ and $e_j$ is<br>$$ \text{cov}\left(e_i, e_j \right ) = \text{cov}\left(y_i, y_j \right ) = 0 $$</p>
<p><strong>SR5.</strong> The variable $x$ is not random and must take at least two different values.</p>
<p><strong>SR6.</strong> (optional) The values of $e$ are normally distributed about their mean<br>$$ e \sim N\left(0, \sigma^2 \right ) $$<br>if the values of $y$ are normally distributed, and vice versa.</p>
<h2 id="Least-Square-Estimators"><a href="#Least-Square-Estimators" class="headerlink" title="Least Square Estimators"></a>Least Square Estimators</h2><p>$$ b_2 = \frac{\sum{\left(x_i-\bar{x} \right )\left(y_i-\bar{y} \right )}}{\sum{\left(x_i-\bar{x} \right )^2}} $$<br>$$ b_1 = \bar{y} - b_2\bar{x} $$</p>
<p><strong>As long as $E\left(e \right ) = 0$ is satisfied</strong>, then both $b_1$ and $b_2$ are unbiased estimators.</p>
<p>$$ \text{var}\left(b_1 \right ) = \sigma^2\left[\frac{\sum{x_i^2}}{N\sum\left(x_i - \bar{x} \right )^2} \right ] $$<br>$$ \text{var}\left(b_2 \right ) = \frac{\sigma^2}{\sum{\left(x_i - \bar{x} \right )^2}} $$<br>$$ \text{cov}\left(b_1, b_2 \right ) = \sigma^2\left[\frac{-\bar{x}}{\sum{\left(x_i - \bar{x} \right )^2}} \right ] $$<br>The proof of above <strong>relies on the assumption</strong> $\text{var}\left(e \right ) = \sigma^2$ and $\text{cov}\left(e_i, e_j \right ) = 0$.</p>
<p><strong>GAUSS–MARKOV THEOREM:</strong> Under the assumptions SR1–SR5 of the linear regression model, the estimators $b_1$ and $b_2$ have the smallest variance of all linear and unbiased estimators of $b_1$ and $b_2$. They are the best linear unbiased estimators (BLUE) of $b_1$ and $b_2$.</p>
<!--Chapter 3-->
<p><strong>Estimating Variance of the error term:</strong><br>$$ \hat{e_i} = y_i - \hat{y_i} = y_i - b_1 - b_2x_i $$<br>$$ \hat{\sigma}^2 = \frac{\sum{\hat{e_i}^2}}{N - 2} $$<br>where 2 is the number of parameters.</p>
<p>Plugging $\hat{\sigma}$ into the variance equations of $b_1$ and $b_2$ gives the standard errors, which is<br>$$ \text{se}(b_1) = \sqrt{\widehat{\text{var}\left(b_1 \right )}} $$<br>$$ \text{se}(b_2) = \sqrt{\widehat{\text{var}\left(b_2 \right )}} $$</p>
<h2 id="Interval-Estimation"><a href="#Interval-Estimation" class="headerlink" title="Interval Estimation"></a>Interval Estimation</h2><p>Given<br>$$ b_2 \sim N\left(\beta_2, \frac{\sigma^2}{\sum{\left(x_i - \bar{x} \right )^2}} \right )$$<br>we have<br>$$ Z = \frac{b_2 - \beta_2}{\sqrt{\text{var}\left(b_2 \right )}} \sim N\left(0, 1 \right ) $$<br>Since the true value is unknown, we have<br>$$ t = \frac{b_2 - \beta_2}{\text{se}\left(b_2 \right )} \sim t_{\left(N - 2 \right )} $$</p>
<p>The confidence interval is therefore<br>$$ P\left[b_k - t_{\left(1-\alpha/2, N-2 \right )}\text{se}\left(b_k \right ) \leq \beta_k \leq b_k + t_{\left(1-\alpha/2, N-2 \right )}\text{se}\left(b_k \right )\right ] = 1 - \alpha $$</p>
<h2 id="Hypothesis-Testing"><a href="#Hypothesis-Testing" class="headerlink" title="Hypothesis Testing"></a>Hypothesis Testing</h2><p><strong>Type I error</strong>: Reject the null hypothesis when it is true.<br><strong>Type II error</strong>: Do not reject a null hypothesis that is false.</p>
<p>The level of significance of a test is the probability of committing a <strong>Type I error</strong>. </p>
<p>One-tail $t$-test with alternative “greater than”: reject null hypothesis if $t \geq t_{\left(1 - \alpha, N - 2 \right )}$.<br>One-tail $t$-test with alternative “smaller than”: reject null hypothesis if $t \leq t_{\left(1 - \alpha, N - 2 \right )}$.<br>Two-tail $t$-test with alternative “not equal to”: reject null hypothesis if $t \leq t_{\left(\alpha/2, N - 2 \right )}$ or $t \geq t_{\left(1 - \alpha/2, N - 2 \right )}$.</p>
<p><strong>$p$-VALUE RULE:</strong> Reject the null hypothesis when the $p$-value is less than, or equal to, the level of significance a. That is, if $p \leq a$ then reject H0. If $p &gt; a$ then do not reject $H_0$.</p>
<p>$p$-value is the probability of the alternative hypothesis.</p>
<p><strong>Testing linear combination of parameters:</strong><br>Suppose $\hat{\lambda} = c_1b_1 + c_2b_2$, then<br>$$ \text{var}\left(\hat{\lambda} \right ) = c_1^2\text{var}\left(b_1 \right ) + c_2^2\text{var}\left(b_2 \right ) + 2c_1c_2\text{cov}\left(b_1, b_2 \right ) $$<br>Replacing the unknown variance gives<br>$$ \widehat{\text{var}\left(\hat{\lambda} \right )} = c_1^2\widehat{\text{var}\left(b_1 \right )} + c_2^2\widehat{\text{var}\left(b_2 \right )} + 2c_1c_2\widehat{\text{cov}\left(b_1, b_2 \right )} $$<br>and<br>$$ \text{se}\left(\hat{\lambda} \right ) = \sqrt{\widehat{\text{var}\left(c_1b_1 + c_2b_2 \right )}} $$</p>
<!--Chapter 4-->
<h2 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h2><p>Given $b_1$ and $b_2$ and assumptions SR1-SR5, the best linear unbiased predictor(BLUP) is given by<br>$$ \hat{y}_0 = b_1 + b_2x_0 $$</p>
<p>Define forecast error by<br>$$f = y_0 - \hat{y}_0 = \left(\beta_1 + \beta_2x_0 + e_0 \right ) - \left(b_1 + b_2x_0 \right )$$<br>We have<br>$$ E\left(f \right ) = 0 $$<br>$$ \text{var}\left(f \right ) = \sigma^2\left[1 + \frac{1}{N} + \frac{\left(x_0 - \bar{x} \right )^2}{\sum{\left(x_i - \bar{x} \right)^2}} \right ] $$<br>Replacing $sigma$ by its estimator $\hat{\sigma}$ we have<br>$$ \widehat{\text{var}\left(f \right )} = \hat{\sigma}^2\left[1 + \frac{1}{N} + \frac{\left(x_0 - \bar{x} \right )^2}{\sum{\left(x_i - \bar{x} \right)^2}} \right ] $$<br>and the <strong>standard error of the forecast</strong> is given by<br>$$ \text{se}\left(f \right ) = \sqrt{\widehat{\text{var}\left(f \right)}} $$</p>
<p>Therefore, the prediction interval is given by<br>$$ \hat{y}_0 \pm t_{\left(1 - \alpha/2, N - 2 \right)} $$<br>where 2 is the number of parameters. </p>
<h2 id="Measuring-Goodness-of-Fit"><a href="#Measuring-Goodness-of-Fit" class="headerlink" title="Measuring Goodness-of-Fit"></a>Measuring Goodness-of-Fit</h2><p>$$ y_i = \hat{y}_i + \hat{e}_i $$<br>$$ y_i - \bar{y} = \left(\hat{y}_i - \bar{y} \right ) + \hat{e}_i $$<br>Using the fact that the cross-product terms are zero we obtain<br>$$ \sum{\left(y_i - \bar{y} \right )^2} = \sum{\left(\hat{y}_i - \bar{y} \right )^2} + \sum{\hat{e}_i^2} $$</p>
<ol>
<li>$\sum{\left(y_i - \bar{y} \right )^2}$ = total sum of squares = $SST$.</li>
<li>$\sum{\left(\hat{y}_i - \bar{y} \right )^2}$ = sum of squares due to the regression = $SSR$.</li>
<li>$\sum{\hat{e}_i^2}$ = sum of squares due to error = $SSE$.</li>
</ol>
<p>Therefore we have<br>$$ SST = SSR + SSE $$<br>And the <strong>coefficient of determination</strong>, or $R^2$ is defined to be<br>$$ R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST} $$</p>
<h2 id="Modeling-Issues"><a href="#Modeling-Issues" class="headerlink" title="Modeling Issues"></a>Modeling Issues</h2><p><strong>Choose proper function form:</strong></p>
<ol>
<li>Linear</li>
<li>Quadratic</li>
<li>Cubic</li>
<li>Log-Log</li>
<li>Log-Linear</li>
<li>Linear-Log</li>
</ol>
<p><strong>Jarque–Bera normality test</strong><br>$$ JB = \frac{N}{6}\left(S^2 + \frac{\left(K - 3 \right )^2}{4} \right ) $$<br>where $N$ is the sample size, $S$ is skewness, and $K$ is kurtosis. When the <strong>residuals</strong> are normally distributed, the Jarque–Bera statistic has a <strong>chi-squared distribution</strong> with two degrees of freedom.We reject the hypothesis of normally distributed errors if a calculated value of the statistic <strong>exceeds</strong> a critical value selected from the chi-squared distribution with two degrees of freedom.</p>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/tags/Econometrics/">Econometrics</a>
  </div>

        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:blog.yibingxie.com">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/Algorithms/">Algorithms</a><small>3</small></li>
  
    <li><a href="/tags/C/">C++</a><small>1</small></li>
  
    <li><a href="/tags/Econometrics/">Econometrics</a><small>3</small></li>
  
    <li><a href="/tags/Finance/">Finance</a><small>1</small></li>
  
    <li><a href="/tags/Mathematics/">Mathematics</a><small>1</small></li>
  
    <li><a href="/tags/Notes/">Notes</a><small>1</small></li>
  
    <li><a href="/tags/Projects/">Projects</a><small>1</small></li>
  
    <li><a href="/tags/Statistical-Learning/">Statistical Learning</a><small>4</small></li>
  
  </ul>
</div>


  <div class="widget tag">
<h3 class="title">Resources</h3>
<ul class="entry">
<li><a href="https://onlinecourses.science.psu.edu/stat501" target="_blank" title="PennState STAT 501">Regression Methods</a></li>
<li><a href="https://onlinecourses.science.psu.edu/stat510" target="_blank" title="PennState STAT 510">Time Series Analysis</a></li>
<li><a href="https://onlinecourses.science.psu.edu/stat857" target="_blank" title="PennState STAT 897D">Statistical Learning</a></li>
<li><a href="http://cs229.stanford.edu/materials.html" target="_blank" title="Stanford CS 229">Machine Learning</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B" target="_blank" title="UFLDL">UFLDL</a></li>
<li><a href="http://deeplearning.net/tutorial/" target="_blank" title="deeplearning.net">Deep Learning</a></li>
</ul>
</div>

<div class="widget tag">
<h3 class="title">Resources</h3>
<ul class="entry">
<li><a href="https://onlinecourses.science.psu.edu/stat501" target="_blank" title="PennState STAT 501">Regression Methods</a></li>
<li><a href="https://onlinecourses.science.psu.edu/stat510" target="_blank" title="PennState STAT 510">Time Series Analysis</a></li>
<li><a href="https://onlinecourses.science.psu.edu/stat857" target="_blank" title="PennState STAT 897D">Statistical Learning</a></li>
<li><a href="http://cs229.stanford.edu/materials.html" target="_blank" title="Stanford CS 229">Machine Learning</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B" target="_blank" title="UFLDL">UFLDL</a></li>
<li><a href="http://deeplearning.net/tutorial/" target="_blank" title="deeplearning.net">Deep Learning</a></li>
</ul>
</div>
</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 Yibing Xie
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'yibingxie';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
