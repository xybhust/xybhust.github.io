<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Review of Econometrics (3) | Yibing Xie</title>
  <meta name="author" content="Yibing Xie">
  
  <meta name="description" content="This article summarizes time series regression, which covers chapter 9, 12, 13 and 14 from the Principles of Econometrics 4th Edition.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Review of Econometrics (3)"/>
  <meta property="og:site_name" content="Yibing Xie"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Yibing Xie" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-76123207-1', 'auto');
	ga('send', 'pageview');

</script>


</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Yibing Xie</a></h1>
  <h2><a href="/">A place with no name..</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-09T06:03:36.000Z"><a href="/2016/04/09/eco-review-3/">2016-04-09</a></time>
      
      
  
    <h1 class="title">Review of Econometrics (3)</h1>
  

    </header>
    <div class="entry">
      
        <p>This article summarizes time series regression, which covers chapter 9, 12, 13 and 14 from the <a href="http://www.amazon.com/Principles-Econometrics-R-Carter-Hill/dp/0470626739/ref=sr_1_1?ie=UTF8&amp;qid=1460072940&amp;sr=8-1&amp;keywords=econometrics+principles" target="_blank" rel="external"><em><strong>Principles of Econometrics</strong></em></a> 4th Edition.<br><a id="more"></a><br><!-- Chapter 9--></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>DYNAMIC NATURE OF RELATIONSHIPS</strong></p>
<ol>
<li><strong>Distributed lag model:</strong> a dependent variable $y$ is a function of current and past values of an explanatory variable $x$.<br>$$ y_t = f\left(x_t, x_{t-1}, x_{t-2}, \cdots \right ) $$</li>
<li><strong>Auto regressive distributed lag (ARDL) model:</strong> specify a model with a <strong>lagged dependent variable</strong> as one of the explanatory variables.<br>$$ y_t = f\left(y_{t-1}, x_t, x_{t-1}, x_{t-2}, \cdots \right ) $$</li>
<li>Modeling the continuing impact of change over several periods is <strong>via the error term</strong>.<br>$$ y_t = f\left(x_t \right ) + e_t \qquad e_t = g\left(e_{t-1} \right ) $$<br>we say the errors are <strong>serially correlated</strong> or <strong>autocorrelated</strong>.</li>
</ol>
<p><strong>LEAST SQUARES ASSUMPTIONS</strong><br>An important consequence of using time series data to estimate dynamic relationships is the possible violation of one of our least squares assumptions.<br>$$ \text{cov}\left(y_t, y_s \right ) = \text{cov}\left(e_t, e_s \right ) = 0 \quad \text{for $t \neq s$} $$</p>
<h2 id="Finite-Distributed-Lags"><a href="#Finite-Distributed-Lags" class="headerlink" title="Finite Distributed Lags"></a>Finite Distributed Lags</h2><p>$$ y_t = \alpha + \beta_0x_t + \beta_1x_{t-1} + \beta_2x_{t-2} + \cdots + \beta_qx_{t-q} + e_t $$<br><strong>ASSUMPTIONS</strong></p>
<ol>
<li>$y_t = \alpha + \beta_0x_t + \beta_1x_{t-1} + \beta_2x_{t-2} + \cdots + \beta_qx_{t-q} + e_t \qquad t = q+1,…, T$</li>
<li>$y$ and $x$ are stationary random variables, and $e_t$ is independent of current, past and future values of $x$.</li>
<li>$E\left(e_t \right ) = 0$</li>
<li>$\text{var}\left(e_t \right ) = \sigma^2$</li>
<li>$\text{cov}\left(e_t, e_s \right ) = 0 \quad t \neq s$</li>
<li>$e_t \sim N\left(0, \sigma^2 \right )$</li>
</ol>
<h2 id="Serial-Correlation"><a href="#Serial-Correlation" class="headerlink" title="Serial Correlation"></a>Serial Correlation</h2><p>When $\text{cov}\left(e_t, e_s \right ) = 0 \quad t \neq s$ is violated we say there exist <strong>serial correlation</strong>.</p>
<p><strong>Detecting serial correlation</strong></p>
<ol>
<li><strong>Plot Correlogram of the residuals</strong></li>
<li><p><strong>A Lagrange multilier test</strong><br>Suppose<br>$$ y_t = \alpha + \beta_0x_t + \beta_1x_{t-1} + \beta_2x_{t-2} + \cdots + \beta_qx_{t-q} + e_t $$<br>where the residuals follows a $AR(p)$ autoregressive scheme as follows<br>$$ e_t = \rho_1e_{t-1} + \rho_2e_{t-2} + \cdots + \rho_pe_{t-p} + v_t $$<br>Then<br>\begin{align} y_t &amp;= \alpha + \beta_0x_t + \beta_1x_{t-1} + \beta_2x_{t-2} + \cdots + \beta_qx_{t-q} + \rho_1e_{t-1} + \rho_2e_{t-2} + \cdots + \rho_pe_{t-p} + v_t \\<br>&amp;\approx \alpha + \beta_0x_t + \beta_1x_{t-1} + \beta_2x_{t-2} + \cdots + \beta_qx_{t-q} + \rho_1\hat{e}_{t-1} + \rho_2\hat{e}_{t-2} + \cdots + \rho_p\hat{e}_{t-p} + v_t<br>\end{align}<br>On the othe hand, $y_t = \hat{\alpha} + b_0x_t + b_1x_{t-1} + b_2x_{t-2} + \cdots + b_qx_{t-q} + \hat{e}_t$, rearrange the terms above we have<br>$$ \hat{e}_t = \left(\alpha - \hat{\alpha} \right ) + \left(\beta_0 - b_0 \right )x_t + \cdots + \left(\beta_q - b_q \right )x_{t-q} + \rho_1\hat{e}_{t-1} + \rho_2\hat{e}_{t-2} + \cdots + \rho_p\hat{e}_{t-p} + v_t $$<br>Because the difference terms are centered around zero, if the above equation is a regression with significant explanatory power, that power will come from the lagged error terms.</p>
<p>If $H_0$: $\rho_i = 0$ is true, then $LM = TR^2$ has an approximate $\chi_{(p)}^2$ distribution where $T$ and $R^2$ are the sample size and goodness-of-fit statistic,respectively, from least squares estimation above.</p>
</li>
<li><p><strong>The Durbin-Watson test</strong><br>$$ d = \frac{\sum_{t=2}^{T}\left(\hat{e}_t - \hat{e}_{t-1} \right )^2}{\sum_{t=1}^{T}\hat{e}_t^2} $$<br>The <strong>null hypothesis</strong> is that there is <strong>no</strong> autocorrelation.</p>
<p>The sample correlogram and the Lagrange multiplier test are two <strong>large sample</strong> tests for serially correlated errors while the Durbin-Watson test does not rely on a large sample approximation.However, the latter only works for lag-1 autocorrelation.</p>
</li>
</ol>
<h2 id="Consequences-and-Solutions"><a href="#Consequences-and-Solutions" class="headerlink" title="Consequences and Solutions"></a>Consequences and Solutions</h2><ol>
<li>The least squares estimator is <strong>still a linear unbiased</strong> estimator, but it is <strong>no longer best</strong>. If we are able to correctly model the autocorrelated errors, then there exists an alternative estimator with a lower variance.</li>
<li>The formulas for the <strong>standard errors</strong> usually computed for the least squares estimator are <strong>no longer correct</strong>, and hence confidence intervals and hypothesis tests that use these standard errors may be misleading.</li>
</ol>
<p><strong>Solutions:</strong> </p>
<ol>
<li><p>Using <strong>HAC (heteroskedasticity and autocorrelation consistent) standard errors</strong>. Keep the terms which were disregarded due to assuming $\text{cov}\left(e_t, e_s \right ) = 0$. </p>
<p>By using HAC standard errors with least squares, we can <strong>avoid having to specify the precise nature</strong> of the autocorrelated error model that is required for an alternative estimator with a lower variance. For the HAC standard errors to be valid, we need to <strong>assume that the autocorrelations go to zero</strong> as the time between observations increases (a condition necessary for stationarity), and we need <strong>a large sample</strong>, but it is not necessary to make a precise assumption about the autocorrelated error model.</p>
</li>
<li>Use $ARMA(p, q)$ model to specify the form of the error term.</li>
</ol>
<h2 id="Autoregressive-Distributed-Lag-Models-ARDL"><a href="#Autoregressive-Distributed-Lag-Models-ARDL" class="headerlink" title="Autoregressive Distributed Lag Models (ARDL)"></a>Autoregressive Distributed Lag Models (ARDL)</h2><p>$$ y_t = \delta + \theta_1y_{t-1} + \cdots + \theta_py_{t-p} + \delta_0x_t + \delta_1x_{t-1}  + \cdots + \beta_qx_{t-q} + v_t $$<br>Choose proper $p$ and $q$ to make $v_t$ white noise based on $AIC$ and $BIC$ criteria.<br><!--Chapter 12--></p>
<h2 id="Stationary-and-Nonstationary"><a href="#Stationary-and-Nonstationary" class="headerlink" title="Stationary and Nonstationary"></a>Stationary and Nonstationary</h2><p><strong>Stationary time series:</strong></p>
<ol>
<li>$E\left(y_t \right ) = \mu$</li>
<li>$\text{var}\left(y_t \right ) = \sigma^2$</li>
<li><p>$\text{cov}\left(y_t, y_{t-s} \right ) = \text{cov}\left(y_t, y_{t+s} \right ) = \gamma_s$</p>
<p>That is, stationary series have the property of <strong>mean reversion</strong>.</p>
</li>
</ol>
<p><strong>Random walk:</strong> $y_t = y_{t-1} + v_t$<br><strong>Random walk with drift:</strong> $y_t = \alpha + y_{t-1} + v_t$<br><strong>Random walk with drift and trend:</strong> $y_t = \alpha + \delta t + y_{t-1} + v_t$</p>
<p><strong>Spurious regression:</strong> there is a danger of obtaining <strong>apparently significant</strong> regression results from unrelated data when nonstationary series are used in regression analysis.</p>
<p><strong>Prewhitening:</strong> when checking cross-correlation analysis between two time series, at least on of them has to be <strong>white noise</strong>.</p>
<p><strong>Unit Root Tests for Stationarity:</strong> Dickey-Fuller test with <strong>null</strong> hypothesis stating that the series is <strong>nonstationary</strong>.</p>
<h2 id="Cointegration"><a href="#Cointegration" class="headerlink" title="Cointegration"></a>Cointegration</h2><p>If $y_t$ and $x_t$ are nonstationary $I(1)$ variables, and $e_t = y_t - \beta_1 - \beta_2x_t$ is a <strong>stationary $I(0)$ process</strong>. In this case $y_t$ and $x_t$ are said to be <strong>cointegrated</strong>. Cointegration implies that $y_t$ and $x_t$ share similar stochastic trends, and, since the difference et is stationary, they never diverge too far from each other.</p>
<p>A natural way to test whether $y_t$ and $x_t$ are cointegrated is to test whether the errors $e_t = y_t - \beta_1 - \beta_2x_t$ are stationary using <strong>Dickey-Fuller test</strong>.</p>
<p>Generally, if $\left\{\mathbf{X}_t \right \}$ is nonstationary, but $\left\{\triangledown^d\mathbf{X}_t \right \}$ is stationary, we say the series is integrated with order $d$. However, it may also be that for some vector $\mathbf{\alpha}$, the (univariate!) time series $\left\{\mathbf{\alpha}^T\mathbf{X}_t \right \}$ is integrated of order less than $d$. Such a process is called cointegrated with vector $\mathbf{\alpha}$.</p>
<h2 id="The-Error-Correction-Model"><a href="#The-Error-Correction-Model" class="headerlink" title="The Error Correction Model"></a>The Error Correction Model</h2><p>Start with a general model that contains lags of $y$ and $x$, namely the autoregressive distributed lag (ARDL) model, except that now the variables are nonstationary:<br>$$ y_t = \delta + \theta_1y_{t-1} + \delta_0x_t + \delta_1x_{t-1} + v_t $$<br>For simplicity, we shall consider lags up to order one, but the following analysis holds for any order of lags. Now recognize that <strong>if y and x are cointegrated</strong>, it means that there is a long run relationship between them. To derive this exact relationship, we set $y_t = y_{t-1} = y$, $x_t = x_{t-1} = x$ and $v_t = 0$ and then, imposing this concept in the ARDL, we obtain<br>$$ y\left(1 - \theta_1 \right ) = \delta + \left(\delta_0 + \delta_1 \right )x $$<br>This equation can be written as $y = \beta_1 + \beta_2x$, where $\beta_1 = \delta/\left(1 - \theta_1 \right )$ and $\beta_2 = \left(\delta_0 + \delta_1 \right )/\left(1 - \theta_1 \right )$. To repeat, we have now derived the implied cointegrating relationship between $y$ and $x$; alternatively, we have derived the long-run relationship that holds between the two $I(1)$ variables.</p>
<p>First, add the term, $-y_{t-1}$, to both sides of the equation:<br>$$ y_t - y_{t-1} = \delta + (\theta_1 - 1)y_{t-1} + \delta_0x_t + \delta_1x_{t-1} + v_t $$<br>Second, add the term $-\delta_0x_{t-1} + \delta_0x_{t-1}$ to the right-hand side to obtain<br>\begin{align}<br>\Delta y_t &amp;= \delta + (\theta_1 - 1)y_{t-1} + \delta_0(x_t - x_{t-1}) + (\delta_0 + \delta_1)x_{t-1} + v_t \\<br>&amp;= (\theta_1 - 1)\left(\frac{\delta}{(\theta_1 - 1)} + y_{t-1} + \frac{(\delta_0 + \delta_1)}{(\theta_1 - 1)}x_{t-1} \right ) + \delta_0\Delta x_t + v_t \\<br>&amp;= -\alpha\left(y_{t-1} - \beta_1 - \beta_2x_{t-1} \right ) + \delta_0\Delta x_t + v_t<br>\end{align}<br>where $\alpha = (1 - \theta_1)$. As you can see, the expression <strong>in parenthesis is the cointegrating relationship</strong>. In other words, we have embedded the cointegrating relationship between $y$ and $x$ in a general ARDL framework.</p>
<p>The last equation is called an <strong>error correction equation</strong> because (a) the expression $y_{t-1} - \beta_1 - \beta_2x_{t-1}$ shows the <strong>deviation</strong> of $y_{t-1}$ from its long run value, $\beta_1 + \beta_2x_{t-1}$. In other words, the “error” in the previous period and (b) the term $\theta_1 - 1$ shows the “correction” of $\Delta y_t$ to the “error”. Empirically we should also find that $\theta_1 - 1 &lt; 0$, which implies that $\theta_1 &lt; 1$.</p>
<p>The error correction model is a very popular model because it allows for the existence of an underlying or fundamental link between variables (<strong>the long-run relationship</strong>) as well as for <strong>short-run adjustments</strong> (i.e. changes) between variables, including adjustments to achieve the cointegrating relationship. It also combines $I(1)$ variables and $I(0)$ variables in the same equation.</p>
<h2 id="Regression-When-There-Is-No-Cointegration"><a href="#Regression-When-There-Is-No-Cointegration" class="headerlink" title="Regression When There Is No Cointegration"></a>Regression When There Is No Cointegration</h2><ol>
<li><strong>Difference stationary:</strong> use differenced variables for regression.</li>
<li><strong>Trend stationary:</strong> remove the effect of the deterministic (constant and trend) components, the residual term is then stationary. (classical decomposition)</li>
</ol>
<!--Chapter 13-->
<h2 id="Vector-Error-Correction-and-Vector-Autoregressive-Models"><a href="#Vector-Error-Correction-and-Vector-Autoregressive-Models" class="headerlink" title="Vector Error Correction and Vector Autoregressive Models"></a>Vector Error Correction and Vector Autoregressive Models</h2><p><strong>VAR model</strong><br>If $y$ and $x$ are stationary $I(0)$ variables, then<br>$$ y_t = \beta_{10} + \beta_{11}y_{t-1} + \beta_{12}x_{t-1} + v_t^y \\<br>x_t = \beta_{20} + \beta_{21}y_{t-1} + \beta_{22}x_{t-1} + v_t^x $$<br>If, however, $y$ and $x$ are nonstationary $I(1)$ and not cointegrated, the VAR is<br>$$ \Delta y_t = \beta_{10} + \beta_{11}\Delta y_{t-1} + \beta_{12}\Delta x_{t-1} + v_t^{\Delta y} \\<br>\Delta x_t = \beta_{20} + \beta_{21}\Delta y_{t-1} + \beta_{22}\Delta x_{t-1} + v_t^{\Delta x} $$<br>To recap: the VAR model is a general framework to describe the dynamic interrelationship <strong>between stationary variables</strong>.</p>
<p><strong>VEC model</strong><br>If $y$ and $x$ are $I(1)$ and cointegrated, then we need to modify the system of equations to allow for the <strong>cointegrating relationship</strong> between the $I(1)$ variables.<br>$$ \Delta y_t = \alpha_{10} + \alpha_{11}\left(y_{t-1} - \beta_0 - \beta_1x_{t-1} \right ) + v_t^y \\<br>\Delta x_t = \alpha_{20} + \alpha_{21}\left(y_{t-1} - \beta_0 - \beta_1x_{t-1} \right ) + v_t^x $$<br>which can expand as<br>$$ y_t = \alpha_{10} + (\alpha_{11} + 1)y_{t-1} - \alpha_{11}\beta_0 - \alpha_{11}\beta_1x_{t-1} + v_t^y \\<br>x_t = \alpha_{20} + \alpha_{21}y_{t-1} - \alpha_{21}\beta_0 - (\alpha_{21}\beta_1 - 1)x_{t-1} + v_t^y $$</p>
<p>Therefore the VEC model is a special form of the VAR for $I(1)$ variables that are cointegrated.</p>
<p>The idea that the error leads to a correction comes about because of the conditions put on $\alpha_{11}$ and $\alpha_{21}$ to ensure stability, namely $(-1 &lt; \alpha_{11} \leq 0)$ and $(0 \leq \alpha_{21} &lt; 1)$.</p>
<h2 id="Estimating-a-Vector-Error-Correction-Model"><a href="#Estimating-a-Vector-Error-Correction-Model" class="headerlink" title="Estimating a Vector Error Correction Model"></a>Estimating a Vector Error Correction Model</h2><p><strong>Two-step least squares procedure</strong><br>First, use least squares to estimate the cointegrating relationship $y_t = \beta_0 + \beta_1x_t + e_t$ and generate the lagged residuals $\hat{e}_{t-1} = y_{t-1} - b_1 - b_2x_{t-1}$. Second, use least squares to estimate the equations:<br>$$ \Delta y_t = \alpha_{10} + \alpha_{11}\hat{e}_{t-1} + v_t^y \\<br>\Delta x_t = \alpha_{20} + \alpha_{21}\hat{e}_{t-1} + v_t^x $$</p>
<p><strong>Special notes</strong>: we should <strong>not mix</strong> stationary and nonstationary variables in the same equation.</p>
<h2 id="Time-Varying-Volatility-and-ARCH-Models"><a href="#Time-Varying-Volatility-and-ARCH-Models" class="headerlink" title="Time-Varying Volatility and ARCH Models"></a>Time-Varying Volatility and ARCH Models</h2><p>Time-varying means the conditional variance that changes over time.<br><strong>ARCH(q) model</strong><br>$$ y_t = \Phi + e_t \\<br>e_t|I_{t-1} \sim N(0, h_t) \\<br>h_t = \alpha_0 + \alpha_1e_{t-1}^2 + \cdots + \alpha_qe_{t-q}^2 $$</p>
<p><strong>Effect of ARCH:</strong> more peaked around the mean and relatively fat tails. Distributions with these properties are said to be <strong>leptokurtic</strong>.</p>
<p><strong>TESTING FOR ARCH EFFECTS:</strong> A Lagrange multiplier (LM) test. First estimate the <strong>mean equation</strong>, then get the residuals.<br>$$ \hat{e}_t^2 = \gamma_0 + \gamma_1\hat{e}_{t-1}^2 + \cdots + \gamma_q\hat{e}_{t-q}^2 + v_t $$<br>If there are ARCH effects, we expect the magnitude of $\hat{e}_t^2$ to depend on its lagged values, and the $R^2$ will be relatively high. The LM test statistic is $(T-q)R^2$ where $T$ is the sample size, $q$ is the number of $\hat{e}_{t-j}^2$ terms on the right-hand side above, and $R^2$ is the coefficient of determination. If the null hypothesis is true, then the test statistic $(T-q)R^2$ is distributed (in large samples) as $\chi_{(q)}^2$, where $q$ is the order of lag, and $(T-q)$ is the number of<br>complete observations. Reject the <strong>null hypothesis</strong> that there is no ARCH effect if $(T-q)R^2 &gt; \chi_{(1-\alpha, q)}^2$</p>
<p><strong>GARCH(p, q) model</strong><br>$$ h_t = \alpha_0 + \alpha_1e_{t-1}^2 + \cdots + \alpha_qe_{t-q}^2 + \beta_1h_{t-1} + \cdots + \beta_ph_{t-p} $$</p>
<h2 id="Announcement"><a href="#Announcement" class="headerlink" title="Announcement"></a>Announcement</h2><p>This is the end of the review series. The motivation for me to write this series is to make it easier for me to refresh my basic econometrics knowledge, which is crucial to my career development. </p>
<p>For more details about time series analysis, check out <a href="http://www.amazon.com/Introduction-Forecasting-Springer-Texts-Statistics/dp/0387953515/ref=sr_1_1?ie=UTF8&amp;qid=1460233460&amp;sr=8-1&amp;keywords=introduction+to+time+series+and+forecasting" target="_blank" rel="external"><strong>Introduction to Time Series and Forecasting</strong></a> and <a href="http://www.amazon.com/Analysis-Financial-Time-Ruey-Tsay/dp/0470414359/ref=sr_1_1?ie=UTF8&amp;qid=1460233531&amp;sr=8-1&amp;keywords=analysis+of+financial+time+series" target="_blank" rel="external"><strong>Analysis of Financial Time Series</strong></a>. Both are terrific books while the former is more theoretical and the latter is more practicable. I might or might not write another review about time series analysis in the future.</p>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/tags/Econometrics/">Econometrics</a>
  </div>

        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:www.yibingxie.com">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/Algorithms/">Algorithms</a><small>3</small></li>
  
    <li><a href="/tags/C/">C++</a><small>1</small></li>
  
    <li><a href="/tags/Econometrics/">Econometrics</a><small>4</small></li>
  
    <li><a href="/tags/Finance/">Finance</a><small>1</small></li>
  
    <li><a href="/tags/Mathematics/">Mathematics</a><small>1</small></li>
  
    <li><a href="/tags/Notes/">Notes</a><small>1</small></li>
  
    <li><a href="/tags/Projects/">Projects</a><small>1</small></li>
  
    <li><a href="/tags/Statistical-Learning/">Statistical Learning</a><small>5</small></li>
  
  </ul>
</div>


  <div class="widget tag">
<h3 class="title">Resources</h3>
<ul class="entry">
<li><a href="https://onlinecourses.science.psu.edu/stat501" target="_blank" title="PennState STAT 501">Regression Methods</a></li>
<li><a href="https://onlinecourses.science.psu.edu/stat510" target="_blank" title="PennState STAT 510">Time Series Analysis</a></li>
<li><a href="https://onlinecourses.science.psu.edu/stat857" target="_blank" title="PennState STAT 897D">Statistical Learning</a></li>
<li><a href="http://cs229.stanford.edu/materials.html" target="_blank" title="Stanford CS 229">Machine Learning</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B" target="_blank" title="UFLDL">UFLDL</a></li>
<li><a href="http://deeplearning.net/tutorial/" target="_blank" title="deeplearning.net">Deep Learning</a></li>
</ul>
</div>

<div class="widget tag">
<h3 class="title">References</h3>
<ul class="entry">
<li><a href="http://www.cplusplus.com/reference/" target="_blank" title="cplusplus.com">C++</a></li>
<li><a href="https://docs.python.org/2.7/" target="_blank" title="Python Docs">Python 2.7</a></li>
<li><a href="http://matplotlib.org/contents.html" target="_blank" title="matplotlib">matplotlib</a></li>
<li><a href="http://scikit-learn.org/stable/user_guide.html" target="_blank" title="scikit-learn">scikit-learn</a></li>
<li><a href="http://statsmodels.sourceforge.net/stable/" target="_blank" title="statsmodels">statsmodels</a></li>
</ul>
</div>
</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 Yibing Xie
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'yibingxie';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
