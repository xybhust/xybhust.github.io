<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Review of Econometrics(2) | Yibing Xie</title>
  <meta name="author" content="Yibing Xie">
  
  <meta name="description" content="This article deals with multiple linear regression, which covers chapter 5-8 from the Principles of Econometrics 4th Edition.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Review of Econometrics(2)"/>
  <meta property="og:site_name" content="Yibing Xie"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Yibing Xie" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-76123207-1', 'auto');
	ga('send', 'pageview');

</script>


</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">Yibing Xie</a></h1>
  <h2><a href="/">A place with no name..</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-08T04:20:48.000Z"><a href="/2016/04/08/eco-review-2/">2016-04-08</a></time>
      
      
  
    <h1 class="title">Review of Econometrics(2)</h1>
  

    </header>
    <div class="entry">
      
        <p>This article deals with multiple linear regression, which covers chapter 5-8 from the <a href="http://www.amazon.com/Principles-Econometrics-R-Carter-Hill/dp/0470626739/ref=sr_1_1?ie=UTF8&amp;qid=1460072940&amp;sr=8-1&amp;keywords=econometrics+principles" target="_blank" rel="external"><em><strong>Principles of Econometrics</strong></em></a> 4th Edition.<br><a id="more"></a><br><!--Chapter 5--></p>
<h2 id="The-Multiple-Regression-Model"><a href="#The-Multiple-Regression-Model" class="headerlink" title="The Multiple Regression Model"></a>The Multiple Regression Model</h2><p><strong>GAUSS–MARKOV ASSUMPTION:</strong><br><strong>MR1.</strong> $y_i = \beta_1 + \beta_2x_{i2} + \cdots + \beta_Kx_{iK} + e_i, i = 1,…,N$<br><strong>MR2.</strong> $E\left(y_i \right ) = \beta_1 + \beta_2x_{i2} + \cdots + \beta_Kx_{iK} \Leftrightarrow E\left(e_i \right ) = 0$<br><strong>MR3.</strong> $\text{var}\left(e \right ) = \sigma^2 = \text{var}\left(y \right )$<br><strong>MR4.</strong> $\text{cov}\left(e_i, e_j \right ) = \text{cov}\left(y_i, y_j \right ) = 0$<br><strong>MR5.</strong> The values of each $x_{ik}$ are not random and are not exact linear functions of the other explanatory variables.<br><strong>MR6.</strong> $y_i \sim N\left(\beta_1 + \beta_2x_{i2} + \cdots + \beta_Kx_{iK}, \sigma^2 \right ) \Leftrightarrow e_i \sim N\left(0, \sigma^2 \right )$</p>
<p><strong>The closed form solution for OLS:</strong><br>$$ \boldsymbol{\hat{\beta} = \left(X^TX \right )^{-1}X^Ty} $$</p>
<p><strong>The closed form solution for GLS:</strong><br>$$ \boldsymbol{\hat{\beta} = \left(X^T\Omega^{-1}X \right )^{-1}X^T\Omega^{-1}y} $$<br>where $\Omega$ is the covariance matrix of the errors.</p>
<p><strong>Unbiased estimator of the error:</strong><br>$$ \hat{\sigma}^2 = \frac{\sum{\hat{e_i}^2}}{N - K} $$<br>where K is the number of $\beta$ parameters.</p>
<p><strong>THE GAUSS–MARKOV THEOREM:</strong> For the multiple regression model, if assumptions MR1–MR5 listed at the beginning of the chapter hold, then the least squares estimators are the best linear unbiased estimators (BLUE) of the parameters.</p>
<p><strong>$t$-statistic:</strong><br>$$t = \frac{b_2 - \beta_2}{\text{se}\left(b_2 \right )} \sim t_{\left(N - K \right )} $$<br>where K is the number of $\beta$ parameters.</p>
<p>Hypothesis testing of parameters as well as linear combination of parameters follows the same routine as the <a href="http://blog.yibingxie.com/2016/04/07/eco-review-1/">simple linear regression</a>.</p>
<!--Chapter 6-->
<h2 id="Testing-Joint-Hypothesis"><a href="#Testing-Joint-Hypothesis" class="headerlink" title="Testing Joint Hypothesis"></a>Testing Joint Hypothesis</h2><p>A null hypothesis with multiple conjectures, expressed with more than one equal sign, is called a <strong>joint hypothesis</strong>. The test used for testing a joint null hypothesis is the $F$-test. A model constrained by hypothesis is called <strong>restricted model</strong>, in constrast with <strong>unrestricted model</strong>.<br>$$ F = \frac{\left(SSE_R - SSE_U \right )/J}{SSE_U/\left(N - K \right )} $$<br>where $J$ is the number of restrictions, $N$ is the number of observations and $K$ is the number of coefficients in the unrestricted model.</p>
<p><strong>If the null hypothesis is true</strong>, then the statistic $F$ has what is called an $F$-distribution with $J$ numerator degrees of freedom and $N - K$ denominator degrees of freedom. Reject the null hypothesis if $F &gt; F_c$.</p>
<p>$F$-test can also be used to test of <strong>the overall significance of the regression model</strong>. In this case, $SSE_R = \sum{\left(y_i - \bar{y} \right )} = SST$, $SSE_U = SSE$, which gives<br>$$ F = \frac{\left(SST - SSE \right )/\left(K - 1 \right )}{SSE/\left(N - K \right )} $$</p>
<p>When testing a single “equality” null hypothesis (a single restriction) <strong>against a “not equal to” alternative hypothesis</strong>, either a $t$-test or an $F$-test can be used; the test outcomes will be identical. This is because the square of a t random variable with df degrees of freedom is an $F$ random variable with 1 degree of freedom in the numerator and df degrees of freedom in the denominator. Namely $ t^2 = F $.</p>
<p><strong>$F$-test for linear constraints:</strong> if the alternative hypothesis is “not equal to”, then $F$-test can be used, otherwise, $t$-test is preferred since the fact that $F = t^2$ makes it impossible to differentiate between left and right tails as $t$-test does.</p>
<h2 id="Model-Specification"><a href="#Model-Specification" class="headerlink" title="Model Specification"></a>Model Specification</h2><p>The possibilities of omitted-variable bias or inflated variances from irrelevant variables mean that it is important to specify an appropriate set of explanatory variables. One can use $t$-test and $F$-test to test different hypothesis.</p>
<p><strong>The adjusted coefficient of determination:</strong><br>$$ \bar{R}^2 = 1 - \frac{SSE/\left(N - K \right )}{SST/\left(N - 1 \right )} $$<br>It penalizes introducing extra vairables.</p>
<p><strong>Akaike information criterion (AIC):</strong><br>$$ AIC = \text{ln}\left(\frac{SSE}{N} \right ) + \frac{2K}{N} $$</p>
<p><strong>Bayesian information criterion (BIC):</strong><br>$$ BIC = \text{ln}\left(\frac{SSE}{N} \right) + \frac{K\text{ln}\left(N \right)}{N} $$<br>The model with the smallest AIC, or the smallest BIC, is preferred.</p>
<p>However, using $AIC$ when n is not many times larger than $K^2$ might cause overfitting, hence we have $AICc$ criteria as follows:<br>$$ AICc = AIC + \frac{2K\left(K + 1 \right )}{N - K - 1} $$<br>If n is many times larger than $K^2$, then the difference becomes negligible.</p>
<p><strong>RESET for model misspecification:</strong> RESET (REgression Specification Error Test) is designed to detect omitted variables and incorrect functional form.<br>Suppose we have identified the following form:<br>$$ \hat{y} = b_1 + b_2x_x + b_3x_3 $$<br>Then, consider the following model:<br>$$ y = \beta_1 + \beta_2x_2 + \beta_3x_3 + \gamma_1\hat{y}^2 + \gamma_1\hat{y}^3 $$<br>And use $F$-test to test whether $\gamma_1$ and $\gamma_1$ are significant. The reason behind this is that Since polynomials can approximate many different kinds of functional forms, if the original functional form is not correct, the polynomial approximation that includes $hat{y}^2$ and $hat{y}^3$ may significantly improve the fit of the model.</p>
<h2 id="Collinearity"><a href="#Collinearity" class="headerlink" title="Collinearity"></a>Collinearity</h2><p>Collinearity refers to a linear relationship between two explanatory variables. In general, whenever there are one or more <strong>exact linear relationships</strong> among the explanatory variables, then the condition of exact collinearity exists. In this case the least squares estimator is not defined.</p>
<p>The more usual case is one in which correlations between explanatory variables might be high, but not exactly one, these circumstances <strong>do not</strong> constitute a violation of least squares assumptions. By the <strong>Gauss–Markov theorem</strong>, the least squares estimator is still the best linear unbiased estimator. However, the variance of the coefficients will be large, which increase the standard error, as a result, it is likely that the usual t-tests will lead to the conclusion that parameter estimates are not significantly different from zero.</p>
<p><strong>IDENTIFYING AND MITIGATING COLLINEARITY:</strong><br>One simple way to detect collinear relationships is to use <strong>sample correlation coefficients</strong> between pairs of explanatory variables.</p>
<p>In cases where collinearity involves multiple variables, run linear regression on these variables:<br>$$ x_2 = a_1x_1 + \cdots + a_Kx_X + e $$<br>If the $R^2$ is high, then the implication is that a large portion of the variation in $x_2$ is explained by variation in the other explanatory variables.</p>
<p>The collinearity problem is that the data <strong>do not contain enough “information”</strong> about the individual effects of explanatory variables to permit us to estimate all the parameters of the statistical model precisely.</p>
<ol>
<li>Take more, and better, sample data.</li>
<li>Drop the variable which causes collinearity.</li>
</ol>
<h2 id="Heteroskedasticity"><a href="#Heteroskedasticity" class="headerlink" title="Heteroskedasticity"></a>Heteroskedasticity</h2><p><strong>Consequences:</strong></p>
<ol>
<li>The least squares estimator is still a linear and unbiased estimator, but it is no longer best. There is another estimator with a smaller variance.</li>
<li>The standard errors usually computed for the least squares estimator are incorrect. Confidence intervals and hypothesis tests that use these standard errors may be misleading.</li>
</ol>
<p><strong>Detecting Heteroskedasticity</strong></p>
<ol>
<li>Residual plots</li>
<li>Lagrange multiplier tests<br>First get<br>$$ y_i = \beta_1 + \beta_2x_{i2} + \cdots + \beta_Kx_{iK} + e_i $$<br>Then run the regression<br>$$ \hat{e}_i^2 = \alpha_1 + \alpha_2z_{i2} + \cdots + \alpha_Sz_{iS} + v_i $$<br>$$ \chi^2 = N\times R^2 \sim \chi_{\left(S - 1 \right )}^2 $$<br>where $R^2$ comes from the regression above.<br>The <strong>null hypothesis</strong> is that there is <strong>no heteroskedasticity</strong>. Reject the null hypothesis if $\chi^2$ exceeds the critical value and conclude that heteroskedasticity exists.</li>
<li>White test<br>Defining the $z$’s as equal to the $x$’s, the squares of the $x$’s, and possibly their cross-products.</li>
<li>Goldfeld-quandt test<br>Divide the total sample into two groups and compare the variance between the two groups.<br>$$ F = \frac{\hat{\sigma}_1^2/\sigma_1^2}{\hat{\sigma}_2^2/\sigma_2^2} \sim F\left(N_1 - K_1, N_2 - K_2 \right ) $$<br>Normally $K_1 = K_2$.<br>$$ H_0: \sigma_1^2 = \sigma_2^2 $$</li>
</ol>
<p><strong>Solutions:</strong></p>
<ol>
<li>Using <strong>heteroskedasticity robust standard errors</strong></li>
<li>Generalized Least Squares(GLS)</li>
</ol>

      
    </div>
    <footer>
      
        
        
  
  <div class="tags">
    <a href="/tags/Econometrics/">Econometrics</a>
  </div>

        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:blog.yibingxie.com">
  </form>
</div>

  

  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/Algorithms/">Algorithms</a><small>2</small></li>
  
    <li><a href="/tags/Econometrics/">Econometrics</a><small>3</small></li>
  
    <li><a href="/tags/Notes/">Notes</a><small>1</small></li>
  
    <li><a href="/tags/Research/">Research</a><small>1</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 Yibing Xie
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'yibingxie';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>
</html>
